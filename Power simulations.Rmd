---
title: "Simulation-based approaches to Type I and II error analyses"
author: "T. Florian Jaeger"
date: "11/19/2019"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: yes
    toc: no
  word_document:
    toc: no
  html_document:
    df_print: paged
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, results = "markup", cache = FALSE,
  fig.align = "center", fig.width = 8)

options(width = 100, mc.cores = 4) 
```

```{r libraries, include=FALSE}
library("tidyverse") 
library("magrittr")     # for pipes
library("broom")        # for elegant handling of model outputs
library("broom.mixed")  # for elegant handling of model outputs for GLMM
library("lme4")         # for GLMMs
library("lmerTest")     # to get p-values for LMEs
library("tictoc")       # for measuring compute time
library("future")       # for parallel computing over multiple corrs
library("furrr")        # for parallelization of map and other purrrs

theme_set(theme_bw())
```

# Preparation for this class
Work through the examples in the Gelman and Hill reading and do Exercises 7.1 & 7.3 (at the end of Ch. 7) and 8.1 (Ch 8). Create an R markdown document with your solutions and send it to the instructor before class.

# The simulation-based approach

For many situations, power can be calculated analytically (see also Gelman and Hill, p. 171 "*Why do we need simulation for predictive inferences?*"). However, often we find ourselves in situations where this isn't possible or at least not trivial (for us). In that case taking a simulation-based approach is simple and powerful. You can use this approach to obtain estimates of power, Type I error, or simply to build intuitions about how an analytical or visualization tool works. Here we focus on power simulations. 

*There are many packages to facilitate this type of simulation in R.* See the end of this handout. In order to build up a conceptual understanding of power simulations first, we go through them step by step rather than to immediately jump to software that does all of these steps for you.


## Power simulations
Power is the estimated proportion of times that you will obtain a significant results, if you repeat an experiment over and over again. As such, power estimates depend on:

 * The analysis approach you intend to use (you can compare the power of multiple approaches). For example, power for trial-level vs. subject-level analyses can/will differ; power for a *t*-test can/will likely differ from power for an LM or Wilcoxon signed rank test, etc.
 
 * Known properties of the data you plan to collect---for example, the amount of data, and how it's structured (e.g., number of trials per subject and number of subjects).
 
 * Assumptions about the ground truth. This includes:
 
   * Assumptions about the **effect sizes** / means.
   
   * Assumptions about the amount of **variability** of the data, possibly at multiple levels of the data (trial-level noise vs. subject-level noise)
   
   * Assumptions about the **type of distribution** the outcome variables take (e.g., Normal, lognormal, bernoulli, poisson, etc. or none of these).
   
   * And and and ... as Gelman and Hill (p. 138) put it "we can complicate [or perhaps better: improve] the model [used to generate our groundruth] in various ways". For example, there might be auto-correlations between trials, subjects might have attentional lapses, experimenters might make mistakes during the item creation etc. For any of these and other possibilities, we can decide whether we want to try to account for them in our power estimates.

*NB:* One of the often *important* choices we can make as to how we obtain the ground truth (which we then feed into the analysis approach for which we are aiming to obtain, e.g., power and Type I error estimates) is which *sources of uncertainty we model*. This at the very least includes sources of uncertainty we, as researchers, have with regard to the ground truth (see, e.g., Gelman and Hill, p. 142 about *uncertainty in regression coefficients* and its link to Bayesian inference on p. 143). For cognitive modeling, another important source of uncertainty is the uncertainty that the *brain/mind* has (e.g., the uncertainty a perceiver has about the true value of various inputs). Thinking carefully about such sources of uncertainty is often critical.

## Parametric vs. non-parametric approaches
There's broadly two types of approaches---just as for most of statistics: parametric and non-parametric approaches. In the parametric approach, we simulate data from parametric distributions (e.g., Normal, binomial, poisson, etc.). Under non-parametric approaches, such as cross-validation or bootstrap, we use previously collected data and resample it---typically with replacement---in order to simulate the 'natural' or 'actual' distribution of the data. Both approaches have their pros and cons (for a comparison of both approaches against reading data, see Burchill & Jaeger, in prep). Here we focus on parametric approaches. This is not an endorsement of those approaches, but rather a choice driven by pedagogical considerations: it allows us to keep our focus on the generative logic behind simulation-based approaches.


# An example
```{r, include=FALSE}
nexp = 1000
nsubj = 24
ntrial = 16

# Information about distribution of accuracy variable
Correct.mu.cond = c("A" = .5, 
                    "B" = .55, 
                    "C" = .81) # proportions
Correct.sd.bySubj = 1

# Information about distribution of RT variable
RT.mu.cond = c("A" = 5.2, 
               "B" = 5.0, 
               "C" = 6.3)
RT.sd.bySubj = .6
RT.sd.resid = .3
```

To familiarize ourselves with power simulations, we will create ```r nexp``` simulated experiments with ```r nsubj``` subjects and ```r ntrial``` trials each for ```r length(RT.mu.cond)``` conditions. This gives us ```r nsubj * ntrial * length(RT.mu.cond)``` observations per simulated experiment. For each trial, we set mean (log-odds of) accuracy and mean (log-transformed) reaction times (RTs). We also specify by-subject variability in the *intercept* of both accuracy and RTs, as well as residual variability in RTs.

**NB:** *How many data sets you need for a stable estimate of power, depends on many factors, including the variability of the data.*

## Make a pipe for data generation

### Defining the *design* (and number of simulated experiments)

As a first step, let's make a pipe that defines our design. The *dplyr* function *crossing* is very helpful in that it makes it trivial to create all unique combinations of an arbitrary number of variables and their values. We're surrounding the pipe by tic() and toc() to measure the compute time. (We might later embed this into a function, but for now let's just make a one-time use pipe.)

```{r design}
tic()
d = 
  crossing(
    exp = 1:nexp,
    condition = c("A","B","C"),
    trial = 1:ntrial,
    subject = 1:nsubj
  ) %>%
  mutate(
    muLogOddsCorrect = qlogis(Correct.mu.cond[condition]),
    muLogRT = RT.mu.cond[condition]
  ) %>%
  group_by(exp, subject) %>%
  mutate(
    muLogOddsCorrect.bySubject = rnorm(1, 0, Correct.sd.bySubj),
    muLogRT.bySubject = rnorm(1, 0, RT.sd.bySubj)
  ) %>%
  ungroup() %>%
  mutate(
    muPCorrect = plogis(muLogOddsCorrect + muLogOddsCorrect.bySubject),
    muLogRT = muLogRT + muLogRT.bySubject
  ) %>%
  mutate_at(c("condition", "subject"), factor)
toc()
```

### *Sample* outcome data 

So far, we've created the design and underlying ground truth of ```r nexp``` experiments. Next, let's *generate* response data---accuracy and RTs---by randomly sampling from the design. We will set a seed for replicability's sake. 

```{r sample}
set.seed(76)

tic()
d %<>%
  mutate(
    correct = rbinom(nrow(.), 1, muPCorrect),
    RT = 100 + round(exp(rnorm(nrow(.), muLogRT, RT.sd.resid)), 0)
  ) 
toc()

# If we want to we could remove the ground truth information now.
# Here we just show the output that without the ground truth (but)
# keep it around b/c why not).
d %>%
  select(-starts_with("mu"))
```

## Visualize example experiments

To get an idea of the type of data we've just generated and how it varies across experiments, let's visualize 8 randomly drawn experiments:

```{r visualize, fig.height=8}
d %>% 
  filter(exp %in% sample(x = unique(exp), size = 8, replace = F)) %>%
  ggplot(aes(x = condition, y = RT, color = subject, group = subject)) +
  geom_point(alpha = .1, position = position_jitter()) +
  stat_summary(fun.y = mean, geom = "line", alpha = .6) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", alpha = .6) +
  facet_wrap(~ exp, nrow = 2) +
  coord_trans(y = "log10") + theme(legend.position = "bottom")
```


## Make a pipe for *analysis* 

For a power analysis, we need to commit to an analysis approach. In the interest of compute time, let's assume we're analyzing the RT data with a linear regression (rather than a linear *mixed-effects* regression, which would more appropriately reflect the hierarchical / repeated-measures nature of our data). We can revisit this assumption once we have gotten a hang of the general approach to power analyses. 

We start with an analysis of the raw (untransformed) RTs, although this deviates from the way we *generated* the data. Later, we might compare this against an approach that also used linear regression but analyzes log-transformed RTs. 

We again use nest and map. This approach is clean and transparent, and thus suitable for pedagogical reasons. It is also reasonably fast, though I note that one can further speed up these simulations through clever use of *purrr*, as Wednesday Bushong showed in her lecture.

```{r analysis}
tic()
d.lm = d %>%
  group_by(exp) %>%
  nest() %>%
  mutate(model = map(data, ~ lm(RT ~ condition, data = .x)))
toc()
```

**NB:** The analysis is *much* slower than the data generation part. This is typically the case and important to remember, as it defines the computational bottleneck for simulations.

It's always a good idea to check whether the code you ran does what you intended (e.g., in terms of running the model over the right data frames). Here's one example model that we've fit.

```{r}
summary(d.lm$model[[1]])
```


## Obtaining power estimates

Now all we have to do is to ask how often an effect of interest reached significance. Say we want to obtain an estimate for the power of our analysis approach with regard to the treatment effect of condition B (against the baseline/control condition A):

```{r power}
tic()
d.lm %>%
  mutate(coefs = map(model, broom::tidy)) %>%
  unnest(coefs) %>%
  filter(term == "conditionB") %>%
  ungroup() %>%
  summarise(
    power = mean(p.value < .05),
    negative.effect = mean(estimate < 0))
toc()
```

## Power as a function of sample and effect size

```{r, include=FALSE}
min_nsubj = nsubj
max_nsubj = nsubj * 3
step_nsubj = nsubj / 2

sim.conds = crossing(
  nsubj = seq(min_nsubj, max_nsubj, step_nsubj),
  RT.mu.A = RT.mu.cond["A"],
  # create an effect (difference of Cond B compared to Cond A) that is 
  # half the size as above, identical, or twice the size:
  RT.mu.B = RT.mu.cond["A"] + 
    (RT.mu.cond["B"] - RT.mu.cond["A"]) * c(.5, 1, 2),
  RT.mu.C = RT.mu.cond["C"]
) 

d = sim.conds %>% 
  crossing(
    exp = 1:nexp,
    condition = c("A","B","C"),
    trial = 1:ntrial,
    subject = 1:nsubj
  )
```

Now that we've seen how to estimate power for a specific set of assumptions, design, effect size, and sample size, let's see how this power estimate changes as a function of effect size and sample size. For example, we might want to be conservative about our effect size estimate and entertain a range of effect sizes. Here we will entertain effect sizes half the size as in the simulation above or twice the size. We might also wonder how many subject we should run in order to reach a certain target power (e.g., 85%). To that end, we will increase the number of subjects from ```r min_nsubj``` in steps of ```r step_nsubj``` up to ```r max_nsubj```.  

Since we're reusing the same code, let's make functions out of the code. And then just call them in one pipe:

```{r, include=FALSE}
my.parameterize = 
  . %>%
    mutate(
      muLogOddsCorrect = qlogis(Correct.mu.cond[condition]),
      muLogRT = RT.mu.cond[condition]
    ) %>%
    group_by(exp, subject) %>%
    mutate(
      muLogOddsCorrect.bySubject = rnorm(1, 0, Correct.sd.bySubj),
      muLogRT.bySubject = rnorm(1, 0, RT.sd.bySubj)
    ) %>%
    ungroup() %>%
    mutate(
      muPCorrect = plogis(muLogOddsCorrect + muLogOddsCorrect.bySubject),
      muLogRT = muLogRT + muLogRT.bySubject
    ) %>%
    mutate_at(c("condition", "subject"), factor)


my.sample =
  . %>%
  mutate(
    correct = rbinom(nrow(.), 1, muPCorrect),
    RT = 100 + round(exp(rnorm(nrow(.), muLogRT, RT.sd.resid)), 0)
  )

my.lm = 
  . %>%
  group_by(exp) %>%
  nest() %>%
  mutate(model = map(data, ~ lm(RT ~ condition, data = .x)))

get.prop.significant = 
  . %>%
  mutate(coefs = map(model, broom::tidy)) %>%
  unnest(coefs) %>%
  filter(term == "conditionB") %>%
  ungroup() %>%
  summarise(
    significant = mean(p.value < .05),
    negative.effect = mean(estimate < 0))
```


## Power is meaningless in the absence of a Type I error estimate

Power can be high simply because we *always* find signficance ... even when there are no differences between conditions. It is thus necessary to also obtain Type I error estimates for the analysis approach. This is of interest in and off itself. But it also allows us to estimates *Type I error-corrected power*. For that we simply obtain the statistic threshold (e.g., the *t*-value) for which the Type I error matches the target Type I error (e.g., .05), and the ask how often we obtain that statistic under the power simulation.

First, we set the effect to zero---here the effect of Condition B vs. A:

```{r}
# Information about distribution of RT variable
RT.mu.cond = c("A" = 5.2, 
               "B" = 5.2, 
               "C" = 6.3)
```
 
 
Next, let's regenerate the data. Since we'll also be reusing the steps necessary to create a design, we're creating a function for that as well. This is the same code we've used for the first power simulation above.

```{r Type I}
my.design = function(Correct.mu.cond, RT.mu.cond) {
  crossing(
    exp = 1:nexp,
    condition = c("A","B","C"),
    trial = 1:ntrial,
    subject = 1:nsubj
  ) 
}

d0 = 
  my.design(RT.mu.cond = RT.mu.cond, Correct.mu.cond = Correct.mu.cond) %>%
  my.parameterize() %>%
  my.sample() 

d0 %>%
  my.lm() %>%
  get.prop.significant()
```

It appears that in this particular case, the linear regression of raw RTs is actually *conservative*.


## Comparison to linear mixed-effects regression

We can use an LME (linear mixed-effects regression a.k.a GLMM with a Gaussian(identity) link) to address the inflated Type I error. But beware that this will be substantially slower. It can also lead to convergence warnings. Here, we are oppressing them, but for actual power and Type I analyses we would have to exclude non-converged models from the analysis (and report the rate of convergence failures separately).

```{r lme, warning=FALSE}
my.lme = 
  . %>%
  group_by(exp) %>%
  nest() %>%
  mutate(model = map(data, ~ lmer(RT ~ condition + 
                                    (1 | subject), 
                                  data = .x, control = 
                                    lmerControl(optimizer = "bobyqa"))))

tic()
d %>%
  my.lme() %>%
  get.prop.significant()
toc()
```

And let's look at the Type I error, too:

```{r, warning=FALSE}
d0 %>%
  my.lme() %>%
  get.prop.significant()
```

The LME is still conservative. Recall that we generated RTs from *log-transformed* means. Let's see how how acknowledging this property in our analysis changes things.

```{r log lme}
my.loglme = 
  . %>%
  group_by(exp) %>%
  nest() %>%
  mutate(model = map(data, ~ lmer(log(RT) ~ condition + 
                                    (1 | subject), 
                                  data = .x, control = 
                                    lmerControl(optimizer = "bobyqa"))))

d %>%
  my.loglme() %>%
  get.prop.significant()

d0 %>%
  my.loglme() %>%
  get.prop.significant()
```


# Refining assumptions

There's a lot of different decisions we could make in estimating the Type I error and power. Here we have focused on diffferent analysis approaches. We could also explore the consequences of assumptions about the data. For example, would anything change if we had missing data or outliers caused by occasional attentional lapses? Would anything change if there was a dependence between the accuracy and RTs of responses? What about floor or ceiling effects, e.g., due to the overall complexity of the experimental task we use?

For adequate estimates, it is important to 1) carefully explore whether the assumptions of the data generation process are likely to match those of the actual data we will obtain in our actual experiment, and to 2) carefully think about whether mismatches between assumptions and the actual data are likely to bias our estimates of the Type I error and power.
 
 
# When you already have data

Above we have assumed that we set the parameters for our simulation arbitrarily. If we already have some pilot data, or if we're interested in calculating the power for an experiment we have already run (we *should* have done that earlier, but ...), then we can plug in the quantities we obtain from our analysis into the design portion of the script shown above. We  might also use the *predict()* function of our analysis (many, but not all, analysis approaches in R have such a function) to generate the predicted means for any new data.

*NB:* Any good analysis of our data will also provide estimates of our uncertainty (e.g., the standard error of our coefficient). If we just use the maximum likelihood **point** estimate of published results, we might over-estimate our effect size and thus our power (because of the 'significance filter': significant results are more likely to get published, Vasishth et al., 2018).


# Existing R packages for power analyses

Above we intentionally went through each of the steps of a simulation. Of course, R provides a number of packages for power simulations. These come in particularly handily when you already have some pilot data (or an entire experiment) and want to calculate power based on those data. 


 1. Use functions that draw simulated examples from any model type for which that is supported. E.g., the *simulate()* function from the arm package by Gelman (the same Gelman as in Gelman and Hill). That combines the design and sample parts of the script (so you only need to apply the analysis part after that). Some of these functions can also draw new data samples while taking into account uncertainty in e.g., the effect size estimates.
 
 2.  Use a power simulation package, e.g., *simr*, to take all the fun away from you.
 
 
# Parallelizing analyses

As we've seen above, the analyses can slow things down quite a bit. One simple and helpful tool in this context is the function *future_map* (and its relatives) from the library *furrr*, which combines the parallelization package *future* and with *purrr*'s map. Future allows us to code in the same way regardless of whether we want to execute code serially, or in parallel over multiple cores on our computer or even over dozens of CPUs on a distributed compute cluster. All we need to change is the 'plan' that determines how future operations are to be scheduled and processed. Thanks to Zach Burchill for help with this section.

```{r}
# Running this might cause a system dialogue to allow R to accept incoming 
# network connections. I denied this request, which might affect the behavior 
# of future. As there seems to be a MAC-specific bug that is currently being
# resolved by the developers of future, I'll leave this for later.
# options(mc.cores = 4)
# plan(multisession, workers = 4)
# 
# tic()
# d.analysis = d %>%
#   group_by(exp) %>%
#   nest() %>%
#   mutate(model = future_map(data, ~ lm(RT ~ condition, data = .x)))
# toc()
```

## Other compute time considerations

There are typically many different ways to achieve the same goal. Some are easier to write, some are easier to read, some are faster, some consume less memory, etc. When we conduct any non-trivial simulation, we move into territory where it pays off to try to be computationally efficient. Here is an example where a small difference in programming makes a big difference in compute time.

We will create 10 simulated experiments with 24 subjects and 60 trials each for 3 conditions. For each trial, we set and simulate accuracy and reaction times. We'll resuse the condition means from the Type I error simulation above.

```{r}
nexp = 10
nsubj = 24
ntrial = 60

d = 
  my.design(RT.mu.cond = RT.mu.cond, Correct.mu.cond = Correct.mu.cond) %>%
  my.parameterize()
```

Now let's sample the responses either rowwise() or as a vector. We're setting a seed to make the compute time comparison maximally informative. First, rowwise:

```{r}
set.seed(1234)

tic()
d %>%
  rowwise() %>%
  mutate(
    correct = rbinom(1, 1, muPCorrect),
    RT = round(exp(rnorm(1, muLogRT, .05)), 3)
  ) %>%
  select(-starts_with("mu"))
toc()
```

And now as a vector:

```{r}
set.seed(1234)

tic()
d %>%
  # Calls mutate over the ungrouped data, thus handing *vectors* 
  # as arguments to rbinom and rnorm
  mutate(
    correct = rbinom(nrow(.), 1, muPCorrect),
    RT = round(exp(rnorm(nrow(.), muLogRT, .05)), 3)
  ) %>%
  select(-starts_with("mu"))
toc()
```

As you can see, the difference in compute time is enormous (but the output is the same). So code wisely!

# References

 * Gelman & Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. (Ch. 7.1-7.2, 8.1-8.3, ~20pp)
 * Vasishth, S., Mertzen, D., JÃ¤ger, L. A., & Gelman, A. 2018. The statistical significance filter leads to overoptimistic expectations of replicability. Journal of Memory and Language, 103, 151-175.

# Session info
```{r session_info, echo=FALSE, results='markup'}
devtools::session_info()
```
